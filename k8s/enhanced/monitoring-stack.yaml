---
# Jaeger for Distributed Tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-all-in-one
  namespace: go-coffee-monitoring
  labels:
    app: jaeger
    component: all-in-one
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
      component: all-in-one
  template:
    metadata:
      labels:
        app: jaeger
        component: all-in-one
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "14269"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.51
        ports:
        - containerPort: 16686  # UI
          name: ui
        - containerPort: 14250  # gRPC
          name: grpc
        - containerPort: 14268  # HTTP
          name: http
        - containerPort: 14269  # Admin/metrics
          name: admin
        - containerPort: 6831   # UDP compact
          name: compact
        - containerPort: 6832   # UDP binary
          name: binary
        - containerPort: 5778   # Config
          name: config
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: METRICS_STORAGE_TYPE
          value: prometheus
        - name: PROMETHEUS_SERVER_URL
          value: http://prometheus:9090
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        readinessProbe:
          httpGet:
            path: /
            port: 14269
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /
            port: 14269
          initialDelaySeconds: 30
          periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: go-coffee-monitoring
  labels:
    app: jaeger
    component: collector
spec:
  type: ClusterIP
  ports:
  - name: grpc
    port: 14250
    targetPort: 14250
    protocol: TCP
  - name: http
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: compact
    port: 6831
    targetPort: 6831
    protocol: UDP
  - name: binary
    port: 6832
    targetPort: 6832
    protocol: UDP
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  selector:
    app: jaeger
    component: all-in-one
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  namespace: go-coffee-monitoring
  labels:
    app: jaeger
    component: query
spec:
  type: ClusterIP
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
    protocol: TCP
  selector:
    app: jaeger
    component: all-in-one
---
# Tempo for Distributed Tracing (Alternative/Additional)
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: go-coffee-monitoring
data:
  tempo.yaml: |
    server:
      http_listen_port: 3200
      grpc_listen_port: 9095
    
    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
        jaeger:
          protocols:
            grpc:
              endpoint: 0.0.0.0:14250
            thrift_http:
              endpoint: 0.0.0.0:14268
            thrift_compact:
              endpoint: 0.0.0.0:6831
        zipkin:
          endpoint: 0.0.0.0:9411
    
    ingester:
      trace_idle_period: 10s
      max_block_bytes: 1_000_000
      max_block_duration: 5m
    
    compactor:
      compaction:
        compaction_window: 1h
        max_block_bytes: 100_000_000
        block_retention: 1h
        compacted_block_retention: 10m
    
    storage:
      trace:
        backend: local
        local:
          path: /tmp/tempo/traces
        wal:
          path: /tmp/tempo/wal
        pool:
          max_workers: 100
          queue_depth: 10000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tempo
  namespace: go-coffee-monitoring
  labels:
    app: tempo
    component: tempo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tempo
      component: tempo
  template:
    metadata:
      labels:
        app: tempo
        component: tempo
    spec:
      containers:
      - name: tempo
        image: grafana/tempo:2.3.1
        args:
        - "-config.file=/etc/tempo/tempo.yaml"
        - "-target=all"
        ports:
        - containerPort: 3200   # HTTP
          name: http
        - containerPort: 9095   # gRPC
          name: grpc
        - containerPort: 4317   # OTLP gRPC
          name: otlp-grpc
        - containerPort: 4318   # OTLP HTTP
          name: otlp-http
        - containerPort: 14250  # Jaeger gRPC
          name: jaeger-grpc
        - containerPort: 14268  # Jaeger HTTP
          name: jaeger-http
        - containerPort: 6831   # Jaeger compact
          name: jaeger-compact
        - containerPort: 9411   # Zipkin
          name: zipkin
        volumeMounts:
        - name: config
          mountPath: /etc/tempo
        - name: storage
          mountPath: /tmp/tempo
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        readinessProbe:
          httpGet:
            path: /ready
            port: 3200
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /ready
            port: 3200
          initialDelaySeconds: 30
          periodSeconds: 30
      volumes:
      - name: config
        configMap:
          name: tempo-config
      - name: storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: tempo
  namespace: go-coffee-monitoring
  labels:
    app: tempo
    component: tempo
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 3200
    targetPort: 3200
    protocol: TCP
  - name: grpc
    port: 9095
    targetPort: 9095
    protocol: TCP
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  - name: jaeger-grpc
    port: 14250
    targetPort: 14250
    protocol: TCP
  - name: jaeger-http
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: jaeger-compact
    port: 6831
    targetPort: 6831
    protocol: UDP
  - name: zipkin
    port: 9411
    targetPort: 9411
    protocol: TCP
  selector:
    app: tempo
    component: tempo
---
# Enhanced Prometheus Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: go-coffee-monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: go-coffee-platform
        environment: production
    
    rule_files:
    - "/etc/prometheus/rules/*.yml"
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    
    - job_name: 'go-coffee-services'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - go-coffee-platform
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    - job_name: 'otel-collector'
      static_configs:
      - targets: ['otel-collector:8889']
    
    - job_name: 'jaeger'
      static_configs:
      - targets: ['jaeger-all-in-one:14269']
    
    - job_name: 'tempo'
      static_configs:
      - targets: ['tempo:3200']
    
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https
    
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics
    
    - job_name: 'kubernetes-cadvisor'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

  alerting_rules.yml: |
    groups:
    - name: go-coffee-platform
      rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 1 minute."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "Error rate is {{ $value }} errors per second."

      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.job }}"
          description: "95th percentile latency is {{ $value }} seconds."

      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently."
